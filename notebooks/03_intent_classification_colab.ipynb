{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a74e5d",
   "metadata": {},
   "source": [
    "# Intent Classification (Colab)\n",
    "\n",
    "Use JSON splits stored in Google Drive. Defaults now target your `Colab Notebooks` folder (`/content/drive/MyDrive/Colab Notebooks`) where your `train.json`, `val.json`, and `test.json` live. Update the paths cell if your layout differs, then run top-to-bottom.\n",
    "\n",
    "- Required files: `train.json`, `val.json`, `test.json` in the selected folder.\n",
    "\n",
    "- Outputs: `best_model.pt`, `test_results.json`, `training_curves.png` saved to `SAVE_DIR`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da197b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'✓ Using device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'  GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')\n",
    "else:\n",
    "    print('⚠️ No GPU detected; training will be slow.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e821d32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab)\n",
    "!pip install -q torch transformers scikit-learn tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7263fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import DistilBertModel, DistilBertTokenizer, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "print('✓ Imports loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ed1315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (run once per session)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths: set to where you stored train/val/test JSONs in Drive\n",
    "\n",
    "# Try common locations under MyDrive (prefers Colab Notebooks)\n",
    "candidates = [\n",
    "    Path('/content/drive/MyDrive/Colab Notebooks'),\n",
    "    Path('/content/drive/MyDrive/Colab Notebook'),  # fallback in case of typo\n",
    "    Path('/content/drive/MyDrive/data/intent_classification/splits'),\n",
    "    Path('/content/drive/MyDrive/intent_data/splits')\n",
    "]\n",
    "\n",
    "DATA_DIR = None\n",
    "for cand in candidates:\n",
    "    required = {split: cand / f\"{split}.json\" for split in [\"train\", \"val\", \"test\"]}\n",
    "    missing = [p for p in required.values() if not p.exists()]\n",
    "    if not missing:\n",
    "        DATA_DIR = cand\n",
    "        break\n",
    "\n",
    "# If not found, raise with guidance\n",
    "if DATA_DIR is None:\n",
    "    checks = []\n",
    "    for cand in candidates:\n",
    "        required = {split: cand / f\"{split}.json\" for split in [\"train\", \"val\", \"test\"]}\n",
    "        missing = [p.name for p in required.values() if not p.exists()]\n",
    "        checks.append(f\"- {cand}: missing {missing}\")\n",
    "    raise FileNotFoundError(\"Could not find train/val/test.json. Checked:\\n\" + \"\\n\".join(checks))\n",
    "\n",
    "SAVE_DIR = Path('/content/drive/MyDrive/intent_experiments/intent_classification_run1')  # change if you prefer a different output location\n",
    "\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"✓ Data dir: {DATA_DIR}\")\n",
    "print(f\"✓ Save dir: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f2a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "CONFIG = {\n",
    "    'model_name': 'distilbert-base-uncased',\n",
    "    'num_labels': 2,\n",
    "    'max_length': 128,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 2e-5,\n",
    "    'epochs': 4,\n",
    "    'warmup_steps': 0,\n",
    "    'weight_decay': 0.01,\n",
    "    'dropout': 0.1,\n",
    "    'random_seed': 42,\n",
    "    'data_base': str(DATA_DIR),\n",
    "    'save_dir': str(SAVE_DIR)\n",
    "}\n",
    "\n",
    "LABEL_MAP = {'in_context': 0, 'out_of_context': 1}\n",
    "LABEL_NAMES = ['in_context', 'out_of_context']\n",
    "\n",
    "torch.manual_seed(CONFIG['random_seed'])\n",
    "np.random.seed(CONFIG['random_seed'])\n",
    "\n",
    "for k, v in CONFIG.items():\n",
    "    print(f'{k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa3362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class IntentDataset(Dataset):\n",
    "    def __init__(self, json_path, tokenizer, max_length=128):\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        self.samples = data['samples']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.label_map = LABEL_MAP\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            sample['text'],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(self.label_map[sample['intent']])\n",
    "        }\n",
    "\n",
    "print('✓ Dataset class ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f0a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer + datasets/dataloaders\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "\n",
    "train_dataset = IntentDataset(DATA_DIR / 'train.json', tokenizer, CONFIG['max_length'])\n",
    "val_dataset = IntentDataset(DATA_DIR / 'val.json', tokenizer, CONFIG['max_length'])\n",
    "test_dataset = IntentDataset(DATA_DIR / 'test.json', tokenizer, CONFIG['max_length'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f'Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc258b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class DistilBertIntentClassifier(nn.Module):\n",
    "    def __init__(self, num_labels=2, model_name='distilbert-base-uncased', dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.distilbert = DistilBertModel.from_pretrained(model_name)\n",
    "        hidden_size = self.distilbert.config.hidden_size\n",
    "        self.classifier = nn.Sequential(nn.Dropout(dropout), nn.Linear(hidden_size, num_labels))\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(pooled)\n",
    "        loss = self.loss_fn(logits, labels) if labels is not None else None\n",
    "        from types import SimpleNamespace\n",
    "        return SimpleNamespace(loss=loss, logits=logits)\n",
    "\n",
    "model = DistilBertIntentClassifier(\n",
    "    num_labels=CONFIG['num_labels'],\n",
    "    model_name=CONFIG['model_name'],\n",
    "    dropout=CONFIG['dropout']\n",
    ")\n",
    "model = model.to(device)\n",
    "print('✓ Model ready on', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b993c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer + scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
    "total_steps = len(train_loader) * CONFIG['epochs']\n",
    "warmup_steps = int(0.1 * total_steps) if CONFIG['warmup_steps'] == 0 else CONFIG['warmup_steps']\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "print(f'Steps: {total_steps}, warmup: {warmup_steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e76668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/eval helpers\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for batch in tqdm(dataloader, desc='Training'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids, attention_mask, labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    return avg_loss, acc, all_preds, all_labels\n",
    "\n",
    "print('✓ Training helpers ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c179a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop\n",
    "\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "best_val_acc, best_epoch = 0, 0\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Training for {CONFIG['epochs']} epochs...\")\n",
    "\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}/{CONFIG['epochs']}\")\n",
    "\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "\n",
    "    val_loss, val_acc, _, _ = evaluate(model, val_loader, device)\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    print(f\"Train loss {train_loss:.4f} acc {train_acc*100:.2f}% | Val loss {val_loss:.4f} acc {val_acc*100:.2f}%\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "\n",
    "        best_val_acc, best_epoch = val_acc, epoch + 1\n",
    "\n",
    "        ckpt = {\n",
    "\n",
    "            'epoch': epoch,\n",
    "\n",
    "            'model_state_dict': model.state_dict(),\n",
    "\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "\n",
    "            'val_acc': val_acc,\n",
    "\n",
    "            'config': CONFIG\n",
    "\n",
    "        }\n",
    "\n",
    "        torch.save(ckpt, SAVE_DIR / 'best_model.pt')\n",
    "\n",
    "        print(f\"✓ Saved new best (val acc {val_acc*100:.2f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Best val acc: {best_val_acc*100:.2f}% (epoch {best_epoch})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b41349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "epochs_range = range(1, CONFIG['epochs'] + 1)\n",
    "ax1.plot(epochs_range, history['train_loss'], 'b-o', label='Train')\n",
    "ax1.plot(epochs_range, history['val_loss'], 'r-o', label='Val')\n",
    "ax1.set_title('Loss')\n",
    "ax1.legend(); ax1.grid(True, alpha=0.3)\n",
    "ax2.plot(epochs_range, [a*100 for a in history['train_acc']], 'b-o', label='Train')\n",
    "ax2.plot(epochs_range, [a*100 for a in history['val_acc']], 'r-o', label='Val')\n",
    "ax2.set_title('Accuracy (%)')\n",
    "ax2.legend(); ax2.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(SAVE_DIR / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('✓ Curves saved to', SAVE_DIR / 'training_curves.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c4285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluation\n",
    "\n",
    "checkpoint = torch.load(SAVE_DIR / 'best_model.pt', map_location=device)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "test_loss, test_acc, test_preds, test_labels = evaluate(model, test_loader, device)\n",
    "\n",
    "print(f\"Test loss: {test_loss:.4f} | Test acc: {test_acc*100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "\n",
    "print(classification_report(test_labels, test_preds, target_names=LABEL_NAMES, digits=4))\n",
    "\n",
    "\n",
    "\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "\n",
    "print(\"\\nConfusion matrix:\")\n",
    "\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5ece10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test results\n",
    "test_results = {\n",
    "    'test_accuracy': float(test_acc),\n",
    "    'test_loss': float(test_loss),\n",
    "    'num_samples': len(test_labels),\n",
    "    'predictions': [int(p) for p in test_preds],\n",
    "    'true_labels': [int(l) for l in test_labels],\n",
    "    'classification_report': classification_report(test_labels, test_preds, target_names=LABEL_NAMES, output_dict=True),\n",
    "    'confusion_matrix': cm.tolist(),\n",
    "    'config': CONFIG\n",
    "}\n",
    "with open(SAVE_DIR / 'test_results.json', 'w') as f:\n",
    "    json.dump(test_results, f, indent=2)\n",
    "print('✓ Saved test_results.json to', SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436f1a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample predictions\n",
    "def predict_intent(text, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, max_length=CONFIG['max_length'], padding='max_length', truncation=True, return_tensors='pt')\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1).item()\n",
    "        conf = probs[0][pred].item()\n",
    "    return LABEL_NAMES[pred], conf\n",
    "\n",
    "samples = [\n",
    "    'I want a black jacket',\n",
    "    'What is the weather today?',\n",
    "    'Show me red dresses',\n",
    "    'Tell me a joke',\n",
    "    'Do you have size medium?',\n",
    "    'How do I cook pasta?'\n",
    "]\n",
    "for s in samples:\n",
    "    intent, conf = predict_intent(s, model, tokenizer, device)\n",
    "    print(f'[{intent}] conf={conf:.3f} | {s}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
